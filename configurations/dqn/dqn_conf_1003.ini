[DQN]
### experiment ###
# nohup python3 -u runner.py configurations/dqn/dqn_conf_1003.ini >> output_c1003.log 2>&1 &
PRE_TRAIN_MODEL_FILE
GPU = 0
# PRE_TRAIN_MODEL_FILE = /home/hongruying/hellor_reply_priority/model/net_c1002_dqn_20180914_152616_20180914_161813.model
# PRE_TRAIN_MODEL_FILE = D:\\software_data\\seekloud\\hellorl\\model\\net_riverraid_dqn_20180914_005216_20180914_104352.model
EPOCH_NUM = 360
EPOCH_LENGTH = 30000

### game env ###
GAME_NAME = riverraid
#GAME_NAME = breakout
ACTION_NUM = 18
OBSERVATION_TYPE = image
CHANNEL = 3
WIDTH = 160
HEIGHT = 210
FRAME_SKIP = 4

### player ###
TRAIN_PER_STEP = 4

### replay buffer ###
PHI_LENGTH = 4
#PHI_LENGTH = 12
BUFFER_MAX = 65536
# BUFFER_MAX = 65536
BEGIN_RANDOM_STEP = 1000


### q-learning ###
DISCOUNT = 0.90
EPSILON_MIN = 0.15
EPSILON_START = 1.0
EPSILON_DECAY = 100000


UPDATE_TARGET_BY_EPISODE_END = 50
UPDATE_TARGET_BY_EPISODE_BEGIN = 5
# update UPDATE_TARGET_DECAY times to get to UPDATE_TARGET_BY_EPISODE_END
UPDATE_TARGET_DECAY = 100

OPTIMIZER = adagrad
#OPTIMIZER = adam
LEARNING_RATE = 0.005
WEIGHT_DECAY = 0.0
GRAD_CLIPPING_THETA = 0.01

POSITIVE_REWARD = 1
NEGATIVE_REWARD = -1

### OTHER ###
MODEL_PATH = /home/hongruying/hellor_reply_priority/model
MODEL_FILE_MARK = c1003_dqn_

REPLAY_PRIORITY = true
DOUBLE_DQN = true
DUELING_DQN = true

EDITED_TIME = 2018-09-14 01:39